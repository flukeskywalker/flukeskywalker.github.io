---
layout: post
title: "ClipUp: A Simple and Powerful Optimizer for Distribution-based Policy Evolution"
tags: projects evolution
categories: projects
permalink: /:categories/:title:output_ext
draft: true
---

<span style="color: gray">by Nihat Engin Toklu, Pawe≈Ç Liskowski and Rupesh Kumar Srivastava</span>

{% marginfigure 'img-humanoid' 'assets/img/2020-06-30-clipup/pgpelib_HumanoidBulletEnv.gif' 'A neural network trained to control the PyBullet Humanoid using PGPE for gradient estimation and ClipUp for optimization.'%}

**ClipUp** is a simple adaptive optimizer that makes it easier to solve certain optimization problems in practice.
<!--more-->
It is generally applicable, but is designed to be specially useful for distribution-based policy search algorithms such as PGPE {% cite sehnke2010 %}, ARS {% cite mania2018 %} and OpenAI-ES {% cite salimans2017 %} by helping in finding good hyperparameters quickly and intuitively.
The technique is simple: use *normalized* gradient descent with momentum, and clip the parameter *updates* (not gradients!).

[Read the full report on arXiv](https://arxiv.org/abs/2008.02387), extending our PPSN 2020 paper.

[Get our code on GitHub](https://github.com/nnaisense/pgpelib), which provides a clean and scalable implementation of PGPE and makes experimenting with this family of algorithms easy.

## Algorithm

{% marginnote "note-algo" "Notice that by setting the hyperparameters appropriately, we recover _normalized gradient descent_. The hyperparameters help us control the algorithm behavior in non-idealized conditions." %}
To get straight to the point, the algorithm is given below.
Using the metaphors of "heavy-ball" momentum, it computes a new velocity of a ball from the current velocity and gradients, that is then added to the current position to obtain the next position.

{% marginnote "note-notation" "Bold symbols denote vectors." %}
$$\textbf{Initialization: } \text{Velocity } \boldsymbol{v_0} = \boldsymbol{0}$$ <br>
$$\textbf{Hyperparameters: }$$ <br>
$$\text{Step size } \alpha, \text{Maximum speed } v^{\text{max}}, \text{Momentum } m$$ <br>
$$\textbf{Input: } \text{Estimated gradient } \nabla f(\boldsymbol{x}_k)$$
<br>

$$\text{// }\textit{Velocity update with normalized gradient}$$ <br>
$$\boldsymbol{v'}_{k+1} \gets m \cdot \boldsymbol{v}_k + \alpha \cdot \big( \nabla f(\boldsymbol{x}_k) \,/\, ||\nabla f(\boldsymbol{x}_k)|| \big)$$ <br>
$$\text{// }\textit{Clip velocity based on norm}$$ <br>
$$\text{if } ||\boldsymbol{v'}_{k+1}|| > v^{\text{max}}$$ <br>
$$\quad$$ $$\boldsymbol{v}_{k+1} \gets v^{\text{max}} \cdot \big( \boldsymbol{v'}_{k+1} \,/\, ||\boldsymbol{v'}_{k+1}|| \big)$$ <br>
$$\text{else }$$ <br>
$$\quad$$ $$\boldsymbol{v}_{k+1} \gets \boldsymbol{v'}_{k+1}$$ <br>
$$\textbf{Return: }\boldsymbol{v}_{k+1}$$

ClipUp comes with a strategy for setting and tuning hyperparameters for control problems.
Start by setting $$m=0.9$$ and $$\alpha=v^{\text{max}}/2$$,{% sidenote "note-alpha" "$$\alpha$$ serves as the _initial speed_ at the first iteration and the rate at which velocity changes after that." %} which leaves $$v^{\text{max}}$$ as the main hyperparameter to tune.
Next, use $$v^{\text{max}}$$ to determine a key hyperparameter of PGPE: $$\boldsymbol{\sigma}$$, the initial standard deviation of Gaussian noise used for estimating the gradient.
Typically $$\boldsymbol{\sigma}$$ should be set such that $$||\boldsymbol{\sigma}||$$ is 10 to 20 times $$v^{\text{max}}$$. 
If computational resources are limited, tune only $$v^{\text{max}}$$, otherwise tune the multipliers above for improvements in performance.{% sidenote "note-hypers" "Depending on the amount of available resources, we recommend tuning the $$||\boldsymbol{\sigma}||$$ multiplier, the $$\alpha$$ multiplier and $$m$$, in order." %}
For more discussion, see Section 4 of our paper. 

## Why ClipUp?

During our experiments with continuous control tasks, we noticed that adjusting hyperparameters often felt like tweaking knobs whose impact on algorithm behavior we couldn't easily predict.{% sidenote "note-sim" "Experiments in our paper use representative tasks based on the MuJoCo and PyBullet physics simulators, but this is a general issue we have faced during practical applications of RL algorithms." %}
When tackling high-dimensional non-convex optimization problems, it is not sufficient to know that an optimizer has good theoretical properties with some ideal hyperparameter settings, since this does not take into account the typical workflow a practitioner might have when solving a problem. 
This workflow usually consists of starting with default hyperparameter settings of the optimizer, and then attempting to adjust both these values and the remaining hyperparameters of the method to improve results.
Many practitioners do not have access to, or can not allocate a large amount of computational resources to perform a large automated hyperparameter search.

### Some Desirable Properties of Optimizers

From the perspective of **optimizers as components of a problem solving strategy**, we consider some questions that a practitioner might ask before deciding to use an optimizer.

{% newthought "Q1:" %} Are there **reasonably good default settings** for the hyperparameters, that often provide a good starting point for my problem class?

{% newthought "Q2:" %} Can I intuitively **interpret the effects of adjusting hyperparameters** on the optimizer behavior?

{% newthought "Q3:" %} Are there **interpretable relationships between hyperparameters**, that help me understand how some hyperparamters should be set in comparison to others?

{% newthought "Q4:" %} Does the optimizer have robustness to **variations in problem definition**, such as different reward functions that I might experiment with for a given task?

We have found that for ClipUp, the answer to these questions is *yes*, more so than other common optimizers.
As a result we can often quickly configure it to learn successful policies.
In particular, it is a great fit for distribution-based search because its hyperparameters can be intuitively interpreted in context of the sampling-based policy gradient estimation algorithm.{% sidenote "sn-pgpe" "We used PGPE, but the arguments apply to any Evolutionary or Randomized Search strategy based on related principles." %}

## Summary of Results

Our paper explains how these useful properties of ClipUp arise and presents several results on a range of simulated continuous control benchmarks, including the challenging PyBullet Humanoid.
The highlights are summarized by the following plots, all using 30 runs aggregated for comparison:

{% maincolumn 'assets/img/2020-06-30-clipup/clipupvsadamhumanoid.png' 'ClipUp **matches or exceeds the performance of Adam** under moderate amount of hyperparameter tuning that one might use in practice.' %}
{% maincolumn 'assets/img/2020-06-30-clipup/humanoidLittlepopClipVsNoclip.png' 'ClipUp can yield **better results using low population sizes** (here 1/8th of original).' %}
{% maincolumn 'assets/img/2020-06-30-clipup/humanoidMoreLrClipVsNoClip.png' 'ClipUp can yield **faster training using higher learning rates** than those optimal for Adam.' %}

For further results, read [our paper](https://arxiv.org/abs/2008.02387)!

### References

{% bibliography --cited %}
