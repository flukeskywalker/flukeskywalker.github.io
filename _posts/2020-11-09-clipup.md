---
layout: post
title: "ClipUp: A Simple and Powerful Optimizer <br>for Distribution-based Policy Evolution"
tags: projects evolution
categories: projects
permalink: /:categories/:title:output_ext
draft: true
---

<span style="color: gray">by Nihat Engin Toklu, Pawe≈Ç Liskowski and Rupesh Kumar Srivastava</span>

{% marginfigure 'img-humanoid' 'assets/img/2020-06-30-clipup/pgpelib_HumanoidBulletEnv.gif' 'A neural network trained to control the PyBullet Humanoid using PGPE for gradient estimation and ClipUp for optimization.'%}

**ClipUp** is a simple adaptive optimizer that makes it easier to solve certain optimization problems in practice.
<!--more-->
It is generally applicable, but is designed to be specially useful for distribution-based policy search algorithms such as PGPE {% cite sehnke2010 %} and variants like ARS {% cite mania2018 %} and OpenAIES {% cite salimans2017 %} and helps in finding good hyperparameters for these algorithms quickly and intuitively.
The technique is simple: use *normalized* gradient descent with momentum, and clip the parameter *updates* (not gradients!).

[Read the full report on arXiv](https://arxiv.org/abs/2008.02387), extending our PPSN 2020 paper.

[Get our code on GitHub](https://github.com/nnaisense/pgpelib), which provides a clean and scalable implementation of PGPE and makes experimenting with this family of algorithms easy.

## Implementation

To get straight to the point, the following code demonstrates how ClipUp's updates are computed.
There are three hyperparameters: `step_size`, `momentum`, and `max_speed`. 
Using the metaphors of "heavy-ball" momentum, the function `compute_update()` computes a new velocity of a ball from the current velocity and gradients, that is then added to the current position to obtain the next iterate.

```python
import numpy as np

def clip(x: np.ndarray, max_magnitude: float) -> np.ndarray:
    """Clip the norm of x to max_magnitude"""
    magnitude = np.linalg.norm(x)
    ratio = max_magnitude / magnitude
    return x * ratio if magnitude > max_magnitude else x

def compute_update(gradient: np.ndarray, velocity: np.ndarray, step_size: float, momentum: float, max_speed: float):
    """Compute a ClipUp update"""
    # normalize the gradient
    normalized_gradient = gradient / np.linalg.norm(gradient)
    # compute velocity with momentum
    velocity = momentum * velocity + normalized_gradient * step_size
    # clip the computed velocity
    return clip(velocity, max_speed)
```

ClipUp comes with a strategy for setting and tuning hyperparameters.
Start by setting `momentum=0.9` and `step_size=max_speed/2`, which leaves `max_speed` as the main hyperparameter to tune.
Next, use `max_speed` to determine a key hyperparameter of PGPE `sigma`: the initial standard deviation of Gaussian noise used for estimating the gradient.
Typically `sigma` should be set such that `norm(sigma)` is 10 to 20 times `max_speed`. 
If computational resources are limited, tune only `max_speed`, otherwise tune the multipliers above for further boosts in performance.{% sidenote "note-hypers" "Depending on the amount of available resources, we recommend tuning the sigma multiplier, the step_size multiplier and momentum, in order." %}
For more discussion, see Section 4 of [our paper](https://arxiv.org/abs/2008.02387). 

## Why ClipUp?

During our experiments with continuous control tasks, we noticed that adjusting hyperparameters often felt like tweaking knobs whose impact on algorithm behavior we couldn't easily predict.{% sidenote "note-sim" "Experiments in our paper use representative tasks based on the MuJoCo and PyBullet physics simulators, but this is a general issue we have faced during practical applications of RL algorithms." %}
When tackling high-dimensional non-convex optimization problems in practice, good theoretical properties of optimizers with idealized hyperparameter settings are valuable but not sufficient, since they don't take into account the typical workflow a practitioner might have. 
This workflow usually consists of starting with default hyperparameter settings of the optimizer, and then attempting to adjust both these values and the remaining hyperparameters of the method to improve results.
Many practitioners do not have access to, or can not allocate a large amount of computational resources to perform large hyperparameter search sweeps.

### Some Desirable Properties of Optimizers

From the perspective of _optimizers as components of a problem solving strategy_, and consider some questions that a practitioner might ask before deciding to use an optimizer.

{% newthought "Q1:" %} Are there *reasonably good default settings* for the hyperparameters, that often provide a good starting point for my problem class?

{% newthought "Q2:" %} Can I intuitively *interpret the effects of adjusting hyperparameters* on the optimizer behavior?

{% newthought "Q3:" %} Are there *interpretable relationships between hyperparameters*, that help me understand how some hyperparamters should be set in comparison to others?

{% newthought "Q4:" %} Does the optimizer have robustness to *variations in problem definition*, such as different reward functions that I might experiment with for a given task?

We have found that for ClipUp, the answer to these question is *yes*, much more so than other common optimizers.
As a result we can often quickly and intuitively configured it to learn successful policies.
In particular, it is a great fit for distribution-based search because its hyperparameters can be interpreted in context of the sampling-based policy gradient estimation algorithm.{% sidenote "sn-pgpe" "We used PGPE, but the arguments apply to any Evolutionary or Randomized Search strategy based on related principles." %}
Read our paper for details about why this is.

### References

{% bibliography --cited %}
